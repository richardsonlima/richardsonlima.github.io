---
layout: post
title:  "A Gentle Journey Through Asymptotic Notation"
date:   2025-09-08 21:00:00
categories: programming
tags: algorithms complexity big-o omega theta
image: /assets/article_images/thomas-t-OPpCbAAKWv8-unsplash.jpg
---

The efficiency of an algorithm is one of the most important aspects in computer science.  
It is not enough for an algorithm to be **correct** â€” it also needs to be **feasible** for large inputs.  

This post is structured like a **mini-book**, where each chapter builds upon the previous one.  
If you skip ahead, you may miss the logical thread â€” so follow the journey step by step.  

---

# Why Efficiency Matters

- Real programs can run correctly but still take years to finish on large inputs.  
- Measuring execution time directly (with a stopwatch) depends on **hardware, compiler, OS**.  
- To reason in a universal way, we need **theoretical analysis**, independent of environment.  

This is where **primitive operations** come into play.  

---

# From Reality to the Theoretical Model

A **primitive operation** is any basic step: assignment, comparison, arithmetic, array access, function call.  
We assume each takes constant time.  

## Example: Counting Operations

Letâ€™s analyze a simple algorithm that finds the maximum element in an array:

```python
def array_max(A):
    current_max = A[0]          # 1 assignment
    for i in range(1, len(A)):  # n-1 iterations
        if A[i] > current_max:  # 1 comparison per iteration
            current_max = A[i]  # up to 1 assignment per iteration
    return current_max          # 1 return
```

In the **worst case**, this algorithm performs approximately `6n` primitive operations.  
Therefore, the execution time grows **linearly** with the input size.

## Detailed Primitive Operation Count for `array_max`

Letâ€™s carefully analyze the number of **primitive operations** (assignments, comparisons, array accesses, increments, etc.) performed by the algorithm.

---

### Step 1 â€“ Initial assignment
```python
current_max = A[0]
```
- **1 assignment** (`current_max`)  
- **1 array access** (`A[0]`)  

âœ”ï¸ **2 operations**

---

### Step 2 â€“ For loop setup
```python
for i in range(1, len(A)):
```
- **1 call** to `len(A)`  
- **1 initialization** of `i`  
- Each iteration (total: `n-1` times):  
  - **1 comparison** (`i < len(A)`)  
  - **1 increment** (`i = i + 1`)  

âœ”ï¸ **â‰ˆ 2(n-1) + 2 operations**

---

### Step 3 â€“ Inside the loop
```python
if A[i] > current_max:
    current_max = A[i]
```

For each of the `n-1` iterations:  
- **1 array access** (`A[i]`)  
- **1 comparison** (`A[i] > current_max`)  

If condition is **true**:  
- **1 assignment** to `current_max`  
- **1 array access** (`A[i]`)  

âœ”ï¸ **2 operations per iteration (best case)**  
âœ”ï¸ **4 operations per iteration (worst case)**

---

### Step 4 â€“ Return
```python
return current_max
```
- **1 return**

âœ”ï¸ **1 operation**

---

### Total Count

Let `n = len(A)`:

- Step 1: `2`  
- Step 2: `2(n-1) + 2`  
- Step 3: between `2(n-1)` and `4(n-1)`  
- Step 4: `1`

**Best case (no updates to `current_max`):**
\[
T(n) = 2 + (2n - 2) + (2n - 2) + 1 = 4n - 1
\]

**Worst case (update every iteration):**
\[
T(n) = 2 + (2n - 2) + (4n - 4) + 1 = 6n - 3
\]

---

### Final Result

- **Best case**: `4n - 1` operations  
- **Worst case**: `6n - 3` operations  
- **Asymptotic complexity**:  
\[
T(n) = Î˜(n)
\]

### âš ï¸ Step-by-step breakdown (another point of view)

According to some points of view, the algorithm could in the worst case execute:

\[ T(n) = 6n - 1 \]

primitive operations.

---

### Two hypothetical analysts

- **Alice (the meticulous analyst):** counts every array access, assignment, and the final failed loop comparison. She concludes the algorithm performs about **6n - 3** operations in the worst case.  

- **Bob (the pragmatic analyst):** uses a slightly different counting convention, not including certain constant terms. He concludes the algorithm performs about **6n - 1** operations in the worst case.  

---

### Why both are correct

- The difference lies in the **counting conventions**:  
  - Whether the final failed comparison in the `for` loop is included.  
  - How the initialization and return are treated.  

- Both Alice and Bob agree that the number of operations is **linear in n**.  
- The divergence (`6n - 3` vs. `6n - 1`) affects only the **constant term**, not the growth rate.  

---

### âœ”ï¸ The common conclusion

No matter which convention you use, the **asymptotic complexity** is the same:

\[
T(n) = Î˜(n)
\]

This means that the **intrinsic growth rate** of `array_max` is linear.  
Hardware or software differences will change only the absolute constant factors, never the growth rate.  

---

# Digging Deeper into Counting

Letâ€™s carefully analyze the primitive operations in `array_max`:  

- **Initialization:** 2 ops  
- **Loop control:** â‰ˆ 2(n-1) ops  
- **Loop body:** 2 (best case) to 4 (worst case) per iteration  
- **Return:** 1 op  

**Best case:**  
\[ T(n) = 4n - 1 \]  

**Worst case:**  
\[ T(n) = 6n - 3 \]  

### Another viewpoint
Some analyses conclude the worst case is `6n - 1`.  
The difference comes from conventions: whether we count the final failed loop test, etc.  

### Two hypothetical analysts
- **Alice (meticulous):** counts everything â†’ `6n - 3`.  
- **Bob (pragmatic):** ignores some constants â†’ `6n - 1`.  

â¡ï¸ Both agree: **linear growth, Î˜(n)**. Constants donâ€™t matter in the big picture.  

---

# Growth Rates

Algorithms have **intrinsic growth rates**. Typical examples:  

- Constant â†’ O(1)  
- Logarithmic â†’ O(log n)  
- Linear â†’ O(n)  
- Quasi-linear â†’ O(n log n)  
- Quadratic â†’ O(nÂ²)  
- Cubic â†’ O(nÂ³)  
- Exponential â†’ O(2â¿)  

On a **log-log plot**, the slope corresponds to growth rate.  
This is why exponential algorithms quickly become impractical.  

---

# Big-O as the Upper Bound

Formal definition:  

![Big-O formal definition](/assets/images/math-01.png)

- Example: `2n + 10 âˆˆ O(n)` (choose `c=3, nâ‚€=10`).  

### Reminders
- â€œf(n) = O(g(n))â€ means **membership**, not equality.  
- Constants and small terms are ignored.  
- Ex: `10nÂ² + 10 log n = O(nÂ²)`.  

---

# Beyond Big-O: Î© and Î˜

- **Î© (Omega):** lower bound. Ex: `12nÂ² - 10 âˆˆ Î©(nÂ²)` but not Î©(nÂ³).  
- **Î˜ (Theta):** both upper and lower bounds. Ex: `6nâ´ + 12nÂ³ + 12 âˆˆ Î˜(nâ´)`.  

### Limit-based view
- If f(n)/g(n) â†’ constant > 0 â†’ f âˆˆ Î˜(g).  
- If f(n)/g(n) â†’ 0 â†’ f âˆˆ o(g).  
- If f(n)/g(n) â†’ âˆ â†’ f âˆˆ Ï‰(g).  

This trio (O, Î©, Î˜) allows us to bound functions precisely.  

---

# Polynomial Functions and Hierarchy

For polynomials: degree dominates.  
\[ p(n) = a_k n^k + â€¦ â‡’ p(n) âˆˆ O(n^k) \]  

Natural hierarchy:  
\[ O(1) < O(log n) < O(n) < O(n log n) < O(nÂ²) < O(nÂ³) < â€¦ < O(2â¿) \]  

This ordering is the backbone of algorithm classification.  

---

# Applying Big-O in Real Algorithms

Rules of thumb:  
- Conditionals â†’ test + slower branch.  
- Consecutive commands â†’ sum (dominated by larger).  
- Nested loops â†’ multiplication.  
- Function calls â†’ cost of function body.  

### Example in Python
```python
# Iterative sum
def sum_iterative(A):
    total = 0
    for x in A:
        total += x
    return total   # Î˜(n)

# Formula sum
def sum_formula(n):
    return n * (n + 1) // 2  # Î˜(1)
```

One traverses the array â†’ O(n). The other uses math â†’ O(1).  

---

# Problem Bounds (UB and LB)

- **Upper bound (UB):** complexity of best known algorithm.  
- **Lower bound (LB):** theoretical minimum for any algorithm.  
- A problem is *solved* when:  
\[ UB(P) âˆˆ Î˜(LB(P)) \]  

This shifts focus from â€œan algorithmâ€ to â€œthe problem itselfâ€.  

---

# The Synthesis: Asymptotic Analysis

Now the puzzle is complete:  

1. Count primitive operations.  
2. Abstract constants away.  
3. Express growth with O/Î©/Î˜.  
4. Place function in the hierarchy.  
5. Apply rules to analyze code.  
6. Relate algorithms to problem bounds.  

â¡ï¸ Asymptotic analysis = **the essence of performance**, beyond machines and languages.  

---

# ğŸš€ The Big Takeaway

- Constants vanish, small terms fade.  
- Growth rate remains the **signature of an algorithm**.  
- Big-O, Î©, Î˜ let us compare algorithms objectively.  
- Mastery here opens the door to sorting, graph algorithms, dynamic programming, and beyond.  

> **In short:** asymptotic notation strips away noise to reveal the algorithmâ€™s true nature.  

---

## ğŸ“ Exercises

- Design a **parallel algorithm** to find the maximum of an array of size `n`.  
- Hint: with `n` processors, time can drop drastically.  
- Bonus: can you design one that uses fewer processors efficiently?  

---

ğŸ“š References:  
- [Cormen, Leiserson, Rivest, Stein. *Introduction to Algorithms*](https://a.co/d/5kl59AV)  
- [Knuth, Donald. *The Art of Computer Programming*](https://a.co/d/ipVU0sr)
